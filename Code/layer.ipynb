{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "def cross_entropy(x):\n",
    "    return -np.log(x)\n",
    "\n",
    "\n",
    "def regularized_cross_entropy(layers, lam, x):\n",
    "    loss = cross_entropy(x)\n",
    "    for layer in layers:\n",
    "        loss += lam * (np.linalg.norm(layer.get_weights()) ** 2)\n",
    "    return loss\n",
    "\n",
    "def categoricaL_crossentropy(y_true, y_pred):                 # CE\n",
    "    return -np.sum(y_true * np.log(y_pred + 10**-100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation\n",
    "\n",
    "def leakyReLU(x, alpha=0.001):\n",
    "\n",
    "    return x * alpha if x < 0 else x\n",
    "\n",
    "\n",
    "def leakyReLU_derivative(x, alpha=0.01):\n",
    "    return alpha if x < 0 else 1\n",
    "\n",
    "\n",
    "def lr_schedule(learning_rate, iteration): # updated code , this will fix the error \n",
    "    if iteration == 0:\n",
    "        return learning_rate\n",
    "    if (iteration >= 0) and (iteration <= 10000):\n",
    "        return learning_rate\n",
    "    if iteration > 10000:\n",
    "        return learning_rate * 0.1\n",
    "    if iteration > 30000:\n",
    "        return learning_rate * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional:                                        # convolution layer using 3x3 filters\n",
    "\n",
    "    def __init__(self, name, num_filters=16, stride=1, size=3, activation=None):\n",
    "        self.name = name\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) * 0.1\n",
    "        self.stride = stride\n",
    "        self.size = size\n",
    "        self.activation = activation\n",
    "        self.last_input = None\n",
    "        self.leakyReLU = np.vectorize(leakyReLU)\n",
    "        self.leakyReLU_derivative = np.vectorize(leakyReLU_derivative)\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.last_input = image                             # keep track of last input for later backward propagation\n",
    "\n",
    "        input_dimension = image.shape[1]                                                # input dimension\n",
    "        output_dimension = int((input_dimension - self.size) / self.stride) + 1         # output dimension\n",
    "\n",
    "        out = np.zeros((self.filters.shape[0], output_dimension, output_dimension))     # create the matrix to hold the\n",
    "                                                                                        # values of the convolution\n",
    "\n",
    "        for f in range(self.filters.shape[0]):              # convolve each filter over the image,\n",
    "            tmp_y = out_y = 0                               # moving it vertically first and then horizontally\n",
    "            while tmp_y + self.size <= input_dimension:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.size <= input_dimension:\n",
    "                    patch = image[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]\n",
    "                    out[f, out_y, out_x] += np.sum(self.filters[f] * patch)\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "        if self.activation == 'relu':                       # apply ReLU activation function\n",
    "            self.leakyReLU(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, din, learn_rate=0.005):\n",
    "        input_dimension = self.last_input.shape[1]          # input dimension\n",
    "\n",
    "        if self.activation == 'relu':                       # back propagate through ReLU\n",
    "           self.leakyReLU_derivative(din)\n",
    "\n",
    "        dout = np.zeros(self.last_input.shape)              # loss gradient of the input to the convolution operation\n",
    "        dfilt = np.zeros(self.filters.shape)                # loss gradient of filter\n",
    "\n",
    "        for f in range(self.filters.shape[0]):              # loop through all filters\n",
    "            tmp_y = out_y = 0\n",
    "            while tmp_y + self.size <= input_dimension:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.size <= input_dimension:\n",
    "                    patch = self.last_input[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]\n",
    "                    dfilt[f] += np.sum(din[f, out_y, out_x] * patch, axis=0)\n",
    "                    dout[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size] += din[f, out_y, out_x] * self.filters[f]\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "        self.filters -= learn_rate * dfilt                  # update filters using SGD\n",
    "        return dout                                         # return the loss gradient for this layer's inputs\n",
    "\n",
    "def get_weights(self):\n",
    "    return np.reshape(self.filters, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:                                              # max pooling layer using pool size equal to 2\n",
    "    def __init__(self, name, stride=2, size=2):\n",
    "        self.name = name\n",
    "        self.last_input = None\n",
    "        self.stride = stride\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.last_input = image                             # keep track of last input for later backward propagation\n",
    "\n",
    "        num_channels, h_prev, w_prev = image.shape\n",
    "        h = int((h_prev - self.size) / self.stride) + 1     # compute output dimensions after the max pooling\n",
    "        w = int((w_prev - self.size) / self.stride) + 1\n",
    "\n",
    "        downsampled = np.zeros((num_channels, h, w))        # hold the values of the max pooling\n",
    "\n",
    "        for i in range(num_channels):                       # slide the window over every part of the image and\n",
    "            curr_y = out_y = 0                              # take the maximum value at each step\n",
    "            while curr_y + self.size <= h_prev:             # slide the max pooling window vertically across the image\n",
    "                curr_x = out_x = 0\n",
    "                while curr_x + self.size <= w_prev:         # slide the max pooling window horizontally across the image\n",
    "                    patch = image[i, curr_y:curr_y + self.size, curr_x:curr_x + self.size]\n",
    "                    downsampled[i, out_y, out_x] = np.max(patch)       # choose the maximum value within the window\n",
    "                    curr_x += self.stride                              # at each step and store it to the output matrix\n",
    "                    out_x += 1\n",
    "                curr_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:                               # fully-connected layer\n",
    "    def __init__(self, name, nodes1, nodes2, activation):\n",
    "        self.name = name\n",
    "        self.weights = np.random.randn(nodes1, nodes2) * 0.1\n",
    "        self.biases = np.zeros(nodes2)\n",
    "        self.activation = activation\n",
    "        self.last_input_shape = None\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "        self.leakyReLU = np.vectorize(leakyReLU)\n",
    "        self.leakyReLU_derivative = np.vectorize(leakyReLU_derivative)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape         # keep track of last input shape before flattening\n",
    "                                                    # for later backward propagation\n",
    "\n",
    "        input = input.flatten()                                 # flatten input\n",
    "\n",
    "        output = np.dot(input, self.weights) + self.biases      # forward propagate\n",
    "\n",
    "        if self.activation == 'relu':                           # apply ReLU activation function\n",
    "            self.leakyReLU(output)\n",
    "\n",
    "        self.last_input = input                     # keep track of last input and output for later backward propagation\n",
    "        self.last_output = output\n",
    "\n",
    "        return output\n",
    "    def backward(self, din, learning_rate=0.005):\n",
    "        if self.activation == 'relu':                             # back propagate through ReLU\n",
    "           self.leakyReLU_derivative(din)\n",
    "\n",
    "        self.last_input = np.expand_dims(self.last_input, axis=1)\n",
    "        din = np.expand_dims(din, axis=1)\n",
    "\n",
    "        dw = np.dot(self.last_input, np.transpose(din))           # loss gradient of final dense layer weights\n",
    "        db = np.sum(din, axis=1).reshape(self.biases.shape)       # loss gradient of final dense layer biases\n",
    "\n",
    "        self.weights -= learning_rate * dw                        # update weights and biases\n",
    "        self.biases -= learning_rate * db\n",
    "\n",
    "        dout = np.dot(self.weights, din)\n",
    "        return dout.reshape(self.last_input_shape)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.reshape(self.weights, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:                                        # dense layer with softmax activation\n",
    "    def __init__(self, name, nodes, num_classes):\n",
    "        self.name = name\n",
    "        self.weights = np.random.randn(nodes, num_classes) * 0.1\n",
    "        self.biases = np.zeros(num_classes)\n",
    "        self.last_input_shape = None\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape         # keep track of last input shape before flattening\n",
    "                                                    # for later backward propagation\n",
    "\n",
    "        input = input.flatten()                                 # flatten input\n",
    "\n",
    "        output = np.dot(input, self.weights) + self.biases      # forward propagate\n",
    "\n",
    "        self.last_input = input                     # keep track of last input and output for later backward propagation\n",
    "        self.last_output = output\n",
    "\n",
    "        return softmax(output)\n",
    "\n",
    "    def backward(self, din, learning_rate=0.005):\n",
    "        for i, gradient in enumerate(din):\n",
    "            if gradient == 0:                   # the derivative of the loss with respect to the output is nonzero\n",
    "                continue                        # only for the correct class, so skip if the gradient is zero\n",
    "\n",
    "            t_exp = np.exp(self.last_output)                      # gradient of dout[i] with respect to output\n",
    "            dout_dt = -t_exp[i] * t_exp / (np.sum(t_exp) ** 2)\n",
    "            dout_dt[i] = t_exp[i] * (np.sum(t_exp) - t_exp[i]) / (np.sum(t_exp) ** 2)\n",
    "\n",
    "            dt = gradient * dout_dt                               # gradient of loss with respect to output\n",
    "\n",
    "            dout = self.weights @ dt                              # gradient of loss with respect to input\n",
    "\n",
    "            # update weights and biases\n",
    "            self.weights -= learning_rate * (np.transpose(self.last_input[np.newaxis]) @ dt[np.newaxis])\n",
    "            self.biases -= learning_rate * dt\n",
    "\n",
    "            return dout.reshape(self.last_input_shape)            # return the loss gradient for this layer's inputs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.reshape(self.weights, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3a18842cd88223d2eea50a134bb96e0584ccfa9a42436ff53b4efa05bbcefc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
